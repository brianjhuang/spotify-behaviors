{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spotify Behaviors Project\n",
    "### Brian Huang, Victor Thai, Annie Fan, Aishani Mohapatra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Sampled Data\n",
    "With our dataset being over 500gbs, it was important that we sampled our data rather than loading it in as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "#start a spark session\n",
    "spark_fp = os.path.join(\"/\", \"Volumes\", \"Marceline Jr.\", \"Spotify Dataset\", \"training_set\")\n",
    "#replace with your filepath\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").csv(spark_fp) #load in the data \n",
    "\n",
    "ids = df.select('session_id').distinct() #get unique user/session ids for sampling. we want to sample by user\n",
    "\n",
    "sampled_users = ids.orderBy(f.rand()).limit(50000) #sample the N users\n",
    "\n",
    "sampled_users_list = list(sampled_users.toPandas()['session_id'])\n",
    "samp_fracs = {key:1 for key in sampled_users_list}\n",
    "#generate the fractions we need to sample from pyspark\n",
    "\n",
    "samp_df = df.sampleBy(\"session_id\", fractions = samp_fracs)\n",
    "samp_df.write.csv(\"./sampled_users_100000.csv\", header = True)\n",
    "#write the file out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sampled_users_100000.csv'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_fp = os.path.join(\"sampled_users_100000.csv\")\n",
    "spark_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv(spark_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = df.toPandas()\n",
    "tf_path_one = os.path.join(\"/\", \"Volumes\", \"Marceline Jr.\", \"Spotify Dataset\", \"track_features\", \"tf_000000000000.csv\")\n",
    "tf_path_two = os.path.join(\"/\", \"Volumes\", \"Marceline Jr.\", \"Spotify Dataset\", \"track_features\", \"tf_000000000001.csv\")\n",
    "\n",
    "track_features_one = pd.read_csv(tf_path_one)\n",
    "track_features_two = pd.read_csv(tf_path_two)\n",
    "\n",
    "track_features = pd.concat([track_features_one, track_features_two])\n",
    "\n",
    "userFeatures = pd.merge(users, track_features, left_on = 'track_id_clean', right_on = 'track_id')\n",
    "nonModifiedFeatures = pd.merge(users, track_features, left_on = 'track_id_clean', right_on = 'track_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(userFeatures.columns)\n",
    "drop = cols[1:25]\n",
    "userFeatures.drop(columns = drop, inplace = True)\n",
    "userFeatures['mode'] = userFeatures['mode'].apply(lambda x: 1 if x == 'major' else 0)\n",
    "\n",
    "features = userFeatures.groupby('session_id').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features.reset_index().drop('session_id', axis = 1)\n",
    "#we wanted to groupby so we would cluster by user avg song features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "cluster = KMeans(n_clusters = 3)\n",
    "cluster.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = userFeatures.groupby('session_id').mean()\n",
    "#we clustered on session id, so y lets us add the labels by user\n",
    "y['Cluster'] = cluster.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "userOne = y[y['Cluster'] == 0]\n",
    "userTwo = y[y['Cluster'] == 1]\n",
    "userThree = y[y['Cluster'] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Clustered Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "userOneFeatures = userOne.merge(nonModifiedFeatures, on = 'session_id', how = 'inner')\n",
    "userTwoFeatures = userTwo.merge(nonModifiedFeatures, on = 'session_id', how = 'inner')\n",
    "userThreeFeatures = userThree.merge(nonModifiedFeatures, on = 'session_id', how = 'inner')\n",
    "\n",
    "userOneFeatures['not_skipped'] = userOneFeatures['not_skipped'].apply(lambda x: 1 if x == True else 0)\n",
    "userTwoFeatures['not_skipped'] = userTwoFeatures['not_skipped'].apply(lambda x: 1 if x == True else 0)\n",
    "userThreeFeatures['not_skipped'] = userThreeFeatures['not_skipped'].apply(lambda x: 1 if x == True else 0)\n",
    "\n",
    "userOneFeatures['premium']= userOneFeatures['premium'].apply(lambda x: 1 if x is True else 0)\n",
    "userOneFeatures['hist_user_behavior_is_shuffle'] = userOneFeatures['hist_user_behavior_is_shuffle'].apply(lambda x: 1 if x is True else 0)\n",
    "\n",
    "userTwoFeatures['premium']= userTwoFeatures['premium'].apply(lambda x: 1 if x is True else 0)\n",
    "userTwoFeatures['hist_user_behavior_is_shuffle'] = userTwoFeatures['hist_user_behavior_is_shuffle'].apply(lambda x: 1 if x is True else 0)\n",
    "\n",
    "userThreeFeatures['premium']= userThreeFeatures['premium'].apply(lambda x: 1 if x is True else 0)\n",
    "userThreeFeatures['hist_user_behavior_is_shuffle'] = userThreeFeatures['hist_user_behavior_is_shuffle'].apply(lambda x: 1 if x is True else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "userOneFeatures.drop(['acousticness_x', 'beat_strength_x', 'bounciness_x',\n",
    "       'danceability_x', 'dyn_range_mean_x', 'energy_x', 'flatness_x',\n",
    "       'instrumentalness_x', 'key_x', 'liveness_x', 'loudness_x',\n",
    "       'mechanism_x', 'mode_x', 'organism_x', 'speechiness_x', 'tempo_x',\n",
    "       'time_signature_x', 'valence_x', 'acoustic_vector_0_x',\n",
    "       'acoustic_vector_1_x', 'acoustic_vector_2_x', 'acoustic_vector_3_x',\n",
    "       'acoustic_vector_4_x', 'acoustic_vector_5_x', 'acoustic_vector_6_x',\n",
    "       'acoustic_vector_7_x', 'Cluster', 'session_position', 'session_length', 'not_skipped', 'context_switch',\n",
    "       'no_pause_before_play', 'short_pause_before_play',\n",
    "       'long_pause_before_play', 'hist_user_behavior_n_seekfwd',\n",
    "       'hist_user_behavior_n_seekback', 'hist_user_behavior_is_shuffle',\n",
    "       'hour_of_day', 'premium', 'context_type',\n",
    "       'hist_user_behavior_reason_start', 'duration', 'release_year',\n",
    "       'us_popularity_estimate'], axis = 1, inplace = True)\n",
    "userTwoFeatures.drop(['acousticness_x', 'beat_strength_x', 'bounciness_x',\n",
    "       'danceability_x', 'dyn_range_mean_x', 'energy_x', 'flatness_x',\n",
    "       'instrumentalness_x', 'key_x', 'liveness_x', 'loudness_x',\n",
    "       'mechanism_x', 'mode_x', 'organism_x', 'speechiness_x', 'tempo_x',\n",
    "       'time_signature_x', 'valence_x', 'acoustic_vector_0_x',\n",
    "       'acoustic_vector_1_x', 'acoustic_vector_2_x', 'acoustic_vector_3_x',\n",
    "       'acoustic_vector_4_x', 'acoustic_vector_5_x', 'acoustic_vector_6_x',\n",
    "       'acoustic_vector_7_x', 'Cluster', 'session_position', 'session_length', 'not_skipped', 'context_switch',\n",
    "       'no_pause_before_play', 'short_pause_before_play',\n",
    "       'long_pause_before_play', 'hist_user_behavior_n_seekfwd',\n",
    "       'hist_user_behavior_n_seekback', 'hist_user_behavior_is_shuffle',\n",
    "       'hour_of_day', 'premium', 'context_type',\n",
    "       'hist_user_behavior_reason_start', 'duration', 'release_year',\n",
    "       'us_popularity_estimate'], axis = 1, inplace = True)\n",
    "userThreeFeatures.drop(['acousticness_x', 'beat_strength_x', 'bounciness_x',\n",
    "       'danceability_x', 'dyn_range_mean_x', 'energy_x', 'flatness_x',\n",
    "       'instrumentalness_x', 'key_x', 'liveness_x', 'loudness_x',\n",
    "       'mechanism_x', 'mode_x', 'organism_x', 'speechiness_x', 'tempo_x',\n",
    "       'time_signature_x', 'valence_x', 'acoustic_vector_0_x',\n",
    "       'acoustic_vector_1_x', 'acoustic_vector_2_x', 'acoustic_vector_3_x',\n",
    "       'acoustic_vector_4_x', 'acoustic_vector_5_x', 'acoustic_vector_6_x',\n",
    "       'acoustic_vector_7_x', 'Cluster', 'session_position', 'session_length', 'not_skipped', 'context_switch',\n",
    "       'no_pause_before_play', 'short_pause_before_play',\n",
    "       'long_pause_before_play', 'hist_user_behavior_n_seekfwd',\n",
    "       'hist_user_behavior_n_seekback', 'hist_user_behavior_is_shuffle',\n",
    "       'hour_of_day', 'premium', 'context_type',\n",
    "       'hist_user_behavior_reason_start', 'duration', 'release_year',\n",
    "       'us_popularity_estimate'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "userOneFeatures.rename(columns = lambda x: x[:-2] if x[-2:] == '_y' else x, inplace = True)\n",
    "userTwoFeatures.rename(columns = lambda x: x[:-2] if x[-2:] == '_y' else x, inplace = True)\n",
    "userThreeFeatures.rename(columns = lambda x: x[:-2] if x[-2:] == '_y' else x, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "userOneFeatures.drop(['track_id_clean', \n",
    "         'skip_1', \n",
    "         'skip_2', \n",
    "         'skip_3',\n",
    "         'hist_user_behavior_reason_end',\n",
    "         'track_id',\n",
    "         'date'], \n",
    "        axis = 1, inplace = True)\n",
    "\n",
    "userTwoFeatures.drop(['track_id_clean', \n",
    "         'skip_1', \n",
    "         'skip_2', \n",
    "         'skip_3',\n",
    "         'hist_user_behavior_reason_end',\n",
    "         'track_id',\n",
    "         'date'], \n",
    "        axis = 1, inplace = True)\n",
    "\n",
    "userThreeFeatures.drop(['track_id_clean', \n",
    "         'skip_1', \n",
    "         'skip_2', \n",
    "         'skip_3',\n",
    "         'hist_user_behavior_reason_end',\n",
    "         'track_id',\n",
    "         'date'], \n",
    "        axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "userOneFeatures.drop(['acoustic_vector_0',\n",
    " 'acoustic_vector_1',\n",
    " 'acoustic_vector_2',\n",
    " 'acoustic_vector_3',\n",
    " 'acoustic_vector_4',\n",
    " 'acoustic_vector_5',\n",
    " 'acoustic_vector_6',\n",
    " 'acoustic_vector_7',\n",
    " 'beat_strength',\n",
    " 'bounciness',\n",
    " 'dyn_range_mean',\n",
    " 'flatness',\n",
    " 'mechanism',\n",
    " 'organism'], axis = 1, inplace = True)\n",
    "\n",
    "userTwoFeatures.drop(['acoustic_vector_0',\n",
    " 'acoustic_vector_1',\n",
    " 'acoustic_vector_2',\n",
    " 'acoustic_vector_3',\n",
    " 'acoustic_vector_4',\n",
    " 'acoustic_vector_5',\n",
    " 'acoustic_vector_6',\n",
    " 'acoustic_vector_7',\n",
    " 'beat_strength',\n",
    " 'bounciness',\n",
    " 'dyn_range_mean',\n",
    " 'flatness',\n",
    " 'mechanism',\n",
    " 'organism'], axis = 1, inplace = True)\n",
    "\n",
    "userThreeFeatures.drop(['acoustic_vector_0',\n",
    " 'acoustic_vector_1',\n",
    " 'acoustic_vector_2',\n",
    " 'acoustic_vector_3',\n",
    " 'acoustic_vector_4',\n",
    " 'acoustic_vector_5',\n",
    " 'acoustic_vector_6',\n",
    " 'acoustic_vector_7',\n",
    " 'beat_strength',\n",
    " 'bounciness',\n",
    " 'dyn_range_mean',\n",
    " 'flatness',\n",
    " 'mechanism',\n",
    " 'organism'], axis = 1, inplace = True)\n",
    "\n",
    "userOneFeatures['mode'] = userOneFeatures['mode'].apply(lambda x: 1 if x == 'major' else 0)\n",
    "userTwoFeatures['mode'] = userTwoFeatures['mode'].apply(lambda x: 1 if x == 'major' else 0)\n",
    "userThreeFeatures['mode'] = userThreeFeatures['mode'].apply(lambda x: 1 if x == 'major' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotifyAPI import Spotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client ID:\n",
      "········\n",
      "Client Secret:\n",
      "········\n"
     ]
    }
   ],
   "source": [
    "s = Spotify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = s.get_playlist_features('Top 50 - USA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from songRecommender import songRecommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = songRecommender(data = userOneFeatures, predict = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'spotify:track:00Blm7zeNqgYLPtW6zg8cj': [(0.9398258408776682,\n",
       "    '17_c91fefd2-fd8e-4f27-a9a4-f30e22790e3b')]},\n",
       " {'spotify:track:0gplL1WMoJ6iYaPgMCL0gX': [(0.9713047616087557,\n",
       "    '21_1735273a-cef0-4c2e-8f4f-e3b232f373d1')]},\n",
       " {'spotify:track:27NovPIUIRrOZoCHxABJwK': [(0.9442731254539111,\n",
       "    '21_ca06c6ed-568f-4263-b478-cb7414ba2fdf')]},\n",
       " {'spotify:track:4R67rQNSbbsR4TdUVOIdez': [(0.95308792646697,\n",
       "    '6_a82686e9-df9e-47b9-8fcb-6876e1b8dfce')]},\n",
       " {'spotify:track:4iN16F8JtVxG2UTzp3avGl': [(0.9774440083591326,\n",
       "    '6_49506d1f-60e7-4b71-a6d6-207377321f9d')]}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similar(model.getPredict(), model.getFeatures())[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
